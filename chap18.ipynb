{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chap18.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPHER4R4AJ8cTIfKuzGDi19"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setting"],"metadata":{"id":"hS3UgySbgETm"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X13h_nURgBuM","executionInfo":{"status":"ok","timestamp":1641966138949,"user_tz":-540,"elapsed":16938,"user":{"displayName":"Hwisung Kwon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizs2pMSWME6appADm19iw3b0G9f_CCWjKda6cL=s64","userId":"05107164275958408212"}},"outputId":"e30bfdff-a5ed-4883-b5d1-2a82520e2685"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["import sys\n","from IPython.display import Image\n","import matplotlib\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import os\n","import warnings\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"Sh7xB_xcgFgS","executionInfo":{"status":"ok","timestamp":1641966141980,"user_tz":-540,"elapsed":3043,"user":{"displayName":"Hwisung Kwon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizs2pMSWME6appADm19iw3b0G9f_CCWjKda6cL=s64","userId":"05107164275958408212"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# 18.1 경험에서 배운다."],"metadata":{"id":"4ccWg1AGgQgo"}},{"cell_type":"markdown","source":["## 18.1.1 강화 학습 이해"],"metadata":{"id":"yAeY6mOggZNS"}},{"cell_type":"markdown","source":["지도 학습은 관리자가 제공하는 레이블된 훈련 샘플에 의존한다.\n","\n","레이블이 없는 처음 본 테스트 샘플에 잘 일반화되는 모델을 훈련하는 것이 목표입니다.\n","\n","지도 학습 모델은 주어진 입력 샘플에 동일한 레이블 또는 값을 할당하는 것을 학습해야 한다. \n","\n","비지도 학습은 군집이나 차원 축소처럼 데이터셋에 내재된 구조를 학습하거나 감지하는 것이 목적입니다.\n","\n","이와 달리 강화 학습은 상호 작용을 통해 학습한다는 개념이다.\n","\n","즉, 모델이 보상 함수를 최대화하기 위해 환경과 상호 작용하면서 학습한다.\n","\n","보상 함수 최대화는 지도 학습에서 비용 함수를 최소화하는 개념과 관련이 있지만 강화 학습에서는 일련의 행동을 학습하기 위한 정답 레이블을 모르며 사전에 정의되어 있지 않다.\n","\n","강화 학습에서 모델은 환경과 상호 작용하기 위해 일련의 행동을 생성한다.\n","\n","이런 상호 작용의 모음을 에피소드라고 부른다.\n","\n","이런 상호 작용을 통해 에이전트는 환경이 제공하는 보상을 모은다.\n","\n","이 보상은 양수 또는 음수일 수 있다.\n","\n","에피소드가 끝날 때까지 에이전트에 제공되지 않기도 한다.\n","\n","체스 게임의 예를 보았을 때 현재 상황을 입력으로 한다.\n","\n","가능한 입력의 개수가 많으면 각 상황이나 상태를 양성 또는 음성으로 레이블하기가 불가능하다.\n","\n","따라서 학습 과정을 구성하기 위해 원하는 결과를 얻었는지 알 수 있는 게임 종료 후에 보상을 제공한다. \n","\n","강화 학습에서는 에이전트, 컴퓨터, 로봇에 무엇을 하라고 가르칠 수 없다.\n","\n","단지 에이전트가 달성해야 할 것을 지정할 수밖에 없다.\n","\n","그다음 시행착오를 통해 얻은 결과를 바탕으로 에이전트의 성공과 실패에 따라 보상을 결정할 수 있다.\n","\n","강화 학습은 복잡한 환경에서의 의사 결정할 때 매우 매력적이다.\n","\n","특히 알려지지 않은 일련의 단계가 문제 해결에 필요할 때이다.\n","\n","강화 학습은 특정 목표를 달성하기 위해 일련의 행동을 학습할 수 있는 강력한 프레임워크를 제공한다.\n","\n","강화 학습 모델 훈련을 어렵게 만드는 한 가지는 이전에 선택했던 행동에 따라 모델의 입력이 달라진다는 점이다.\n","\n","이로 인해 각종 문제가 발생하고 불안정한 훈련을 야기한다.\n","\n","강화 학습에 있는 이런 시퀀스 의존성은 지연 효과를 만든다.\n","\n","타임 스텝 t에 선택한 행동이 언제일지 모르지만 일련의 타임 스텝 뒤에 나타난 미래 보상을 만들어 낸다."],"metadata":{"id":"Y2TtqV8CgbN1"}},{"cell_type":"markdown","source":["## 18.1.2 강화 학습 시스템의 에이전트-환경 인터페이스 정의"],"metadata":{"id":"6KiRFenAmAmD"}},{"cell_type":"markdown","source":["모든 강화 학습 예에서 두 개의 객체를 뚜렷하게 구분할 수 있다.\n","\n","바로 에이전트와 환경이다.\n","\n","에이전트는 행동을 통해 주변 환경과 상호 작용하고 의사 결정 방법을 배우는 객체로 정의된다.\n","\n","에이전트는 행동의 결과로 환경에서 주는 관측과 보상 신호를 받는다.\n","\n","환경은 에이전트를 제외한 모든 것이다.\n","\n","환경은 에이전트와 통신하고 에이전트의 행동에 대한 보상 신호를 결정하고 관측 내용을 전달한다.\n","\n","보상 신호는 에이전트가 환경과 상호 작용하면서 받는 피드백이다.\n","\n","보통 스칼라 값으로 제공되고 음수나 양수이다.\n","\n","보상은 에이전트가 얼마나 잘 수행했는지 알려주는 것이 목적이다.\n","\n","에이전트가 보상을 받는 주기는 작업이나 문제에 따라 다르다."],"metadata":{"id":"wrfAz3NDmD9Y"}},{"cell_type":"code","source":["Image(url='https://git.io/JtTQo', width=700)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":445},"id":"6WuwBg1Ant-T","executionInfo":{"status":"ok","timestamp":1641966142014,"user_tz":-540,"elapsed":77,"user":{"displayName":"Hwisung Kwon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizs2pMSWME6appADm19iw3b0G9f_CCWjKda6cL=s64","userId":"05107164275958408212"}},"outputId":"0c623b55-da9f-4966-a7f9-06957bfbb660"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://git.io/JtTQo\" width=\"700\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# 18.2 강화 학습의 기초 이론"],"metadata":{"id":"vwD2hXOsnu8U"}},{"cell_type":"markdown","source":["## 18.2.1 마르코프 결정 과정"],"metadata":{"id":"zqCimR_Wn9g7"}},{"cell_type":"markdown","source":["일반적으로 강화 학습이 다루는 문제는 마르코프 결정 과정으로 기술할 수 있다.\n","\n","MDP 문제를 푸는 기본 방법은 동적 계획법을 사용하는 것이다.\n","\n","하지만 강화 학습은 동적 계획법에 비해 몇 가지 장점을 제공한다.\n","\n","(가능한 상황의 수가 커지면)"],"metadata":{"id":"5k9zUdken_pP"}},{"cell_type":"markdown","source":["## 18.2.2 마르코프 결정 과정의 수학 공식"],"metadata":{"id":"-_X-GS2joSgg"}},{"cell_type":"code","source":["Image(url='https://git.io/JtTQi', width=700)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"id":"GZ5Pp0FNoVOv","executionInfo":{"status":"ok","timestamp":1641966142017,"user_tz":-540,"elapsed":69,"user":{"displayName":"Hwisung Kwon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizs2pMSWME6appADm19iw3b0G9f_CCWjKda6cL=s64","userId":"05107164275958408212"}},"outputId":"8a6dab78-2fc4-4816-fdb6-0e88d03ac59e"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://git.io/JtTQi\" width=\"700\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["쉽게 말하면 환경 동역학이 결정적이면 전이할 확률은 100%이다."],"metadata":{"id":"fwgGcM_pogto"}},{"cell_type":"markdown","source":["마르코프 과정 시각화\n","\n","유향 순환 그래프로 표현할 수 있다.\n","\n","노트는 각 상태를 나타내고 에지는 상태 간의 전이확률을 의미한다."],"metadata":{"id":"zN6hKWrSpEG7"}},{"cell_type":"code","source":["Image(url='https://git.io/JtTQP', width=700)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":274},"id":"dCpd1h-7qcSt","executionInfo":{"status":"ok","timestamp":1641966142019,"user_tz":-540,"elapsed":67,"user":{"displayName":"Hwisung Kwon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizs2pMSWME6appADm19iw3b0G9f_CCWjKda6cL=s64","userId":"05107164275958408212"}},"outputId":"51adec8a-13cf-4511-a90e-7af9933ef4d7"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://git.io/JtTQP\" width=\"700\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["에피소드 작업 vs 연속적인 작업\n","\n","에이전트가 환경과 상호 작용함에 따라 관측이나 상태의 시퀀스는 궤적을 형성한다.\n","\n","에이전트의 궤적을 시간 t=0에서 시작하고 종료 상태에서 끝나는 부분 궤적으로 나눌 수 있다면 에피소드 작업이라 한다.\n","\n","반면 종료 상태가 끝없이 궤적이 계속되면 이 작업을 연속적인 작업이라고 한다."],"metadata":{"id":"oAYmodMNqdsJ"}},{"cell_type":"markdown","source":["## 18.2.3 강화 학습 용어 : 대가, 정책, 가치 함수"],"metadata":{"id":"F1tAeL2LqzQj"}},{"cell_type":"markdown","source":["대가\n","\n","대가는 에피소드 전체 기간을 통해 누적된 보상이다.\n","\n","R_(t+1)은 시간 t에서 행동을 수행한 후 즉각 얻은 보상이다.\n","\n","즉각적인 보상과 할인 계수를 곱한 것들을 모두 더해 대가를 구할 수 있다.\n","\n","계수는 미래 보상이 현재 시점에서 얼마나 가치 있는지 나타낸다.\n","\n"],"metadata":{"id":"zZe1QCmgq2Ym"}},{"cell_type":"code","source":["# 할인 계수에 대하여\n","\n","Image(url='https://git.io/Jtkcl', width=700)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":420},"id":"Kx7jO7bir3sY","executionInfo":{"status":"ok","timestamp":1641966142023,"user_tz":-540,"elapsed":68,"user":{"displayName":"Hwisung Kwon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizs2pMSWME6appADm19iw3b0G9f_CCWjKda6cL=s64","userId":"05107164275958408212"}},"outputId":"d9cab2ed-31d6-49a0-ec35-6cebee719dad"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://git.io/Jtkcl\" width=\"700\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["정책\n","\n","다음에 선택할 행동을 결정하는 함수로 결정적일 수도 있고 확률적일 수도 있다.\n","\n","확률적 정책은 주어진 상태에서 에이전트가 선택할 수 있는 행동에 대한 확률 분포를 갖는다.\n","\n","학습 과정 동안 에이전트가 더 많은 경험을 얻으면 정책이 바뀔 수 있다.\n","\n","처음에는 에이전트가 모든 행동의 확률이 균등하게 랜덤한 정책으로 시작할 수 있다.\n","\n","그동안 에이전트는 최적의 정책에 가깝게 되도록 정책을 학습할 것이다.\n","\n","최적 정책은 가장 높은 대가를 만드는 정책이다."],"metadata":{"id":"JdGgWXzNr5JE"}},{"cell_type":"markdown","source":["가치 함수\n","\n","가치 함수는 각 상태의 좋음을 측정한다.\n","\n","특정 상태가 얼마나 좋은지 또는 나쁜지를 측정한다.\n","\n","좋음에 대한 기준은 대가를 기반으로 한다.\n","\n","가치 함수를 정책을 따른 후 대가의 기대값으로 정의한다.\n","\n","상태-행동 쌍에 대해 가치를 정의할 수 있는데 이를 행동-가치 함수라고 한다."],"metadata":{"id":"8zy2Q14VshZ-"}},{"cell_type":"markdown","source":["## 18.2.4 벨먼 방정식을 사용한 동적 계획법"],"metadata":{"id":"xeFFabf6sxp6"}},{"cell_type":"markdown","source":["많은 강화 학습 알고리즘의 핵심 요소 중 하낟.\n","\n","가치 함수의 계산을 단순화한다."],"metadata":{"id":"wvm5Yg2qtC2d"}},{"cell_type":"markdown","source":["# 18.3 강화 학습 알고리즘"],"metadata":{"id":"XLqt8HPLuBIx"}},{"cell_type":"code","source":["Image(url='https://git.io/Jtkc4', width=700)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"H6YbFuYFuDj5","executionInfo":{"status":"ok","timestamp":1641966142029,"user_tz":-540,"elapsed":71,"user":{"displayName":"Hwisung Kwon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizs2pMSWME6appADm19iw3b0G9f_CCWjKda6cL=s64","userId":"05107164275958408212"}},"outputId":"8146f9bd-2872-448c-edd1-98c9562adc92"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://git.io/Jtkc4\" width=\"700\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["## 18.3.1 동적 계획법"],"metadata":{"id":"xlrjq-gduIKP"}},{"cell_type":"markdown","source":["다음 가정\n","\n","환경 동역학에 대해 완벽히 알고 있다. 즉, 모든 전이 확률을 안다.\n","\n","에이전트의 상태는 마르코프 성질을 갖는다. 다음 행동과 보상은 오직 현재 상태와 현재 타임 스텝에서 선택한 행동에 의해서만 결정된다.\n","\n","환경 동역학에 대해 완벽히 알고 있다는 가정은 사실 실제 애플리케이션에서는 비현실적이다.\n","\n","두 개의 목표가 있다.\n","\n","1. 진짜 상태 - 가치 함수 구하기. 예측 작업이라고 하며 정책 평가로 구할 수 있다.\n","\n","2. 최적의 가치 함수 찾기. 일반화된 정책 반복을 통해 얻을 수 있다."],"metadata":{"id":"t_9Yhu7_uKkD"}},{"cell_type":"markdown","source":["정책 평가 : 동적 계획법으로 가치 함수 예측\n","\n","추정된 가치 함수로 정책 향상 시키기\n","\n","정책 반복\n","\n","가치 반복"],"metadata":{"id":"_TMla6gZur1B"}},{"cell_type":"markdown","source":["## 18.3.2 몬테카를로를 사용한 강화 학습"],"metadata":{"id":"QKCbSAEVwPck"}},{"cell_type":"markdown","source":["환경 동역학에 대한 정보가 전혀 없다는 가정이다.\n","\n","즉, 환경의 상태 - 전이 확률을 모른다.\n","\n","에이전트는 환경과 상호 작용을 통해 학습해야 한다.\n","\n","MC 방법을 사용할 때 학습 과정은 모의 경험을 기반으로 한다.\n","\n","MC 기반 방법은 에이전트가 환경과 상호 작용할 모의 에피소드를 생성하여 이 문제를 해결한다.\n","\n","MC를 사용한 상태-가치 함수 추정\n","\n","MC를 사용한 행동-가치 함수 추정\n","\n","MC 제어를 사용하여 최적의 정책 찾기\n","\n","정책 향상 : 가치 함수로부터 그리디 정책 계산"],"metadata":{"id":"d6IypEY4wS7A"}},{"cell_type":"markdown","source":["## 18.3.3 시간 차 학습"],"metadata":{"id":"sA3qWLBFw6GK"}},{"cell_type":"markdown","source":["TD 학습\n","\n","환경 동역학과 전이 확률에 대한 지식이 하나도 필요없다.\n","\n","MC와의 차이점은 MC는 총 대가를 계산하기 위해 에피소드가 끝날 때까지 기다려야 하지만 TD는 끝에 도달하기 전에 몇 가지 학습된 성징을 사용해 추정된 가치를 업데이트 할 수 있다.\n","\n","부트스트래핑이라고 한다.\n","\n","TD 예측\n","\n","온-폴리시 TD 제어(SARSA)\n","\n","오프-폴리시 TD 제어(Q-러닝)"],"metadata":{"id":"CNpeiBFgw8Zd"}},{"cell_type":"markdown","source":["# 18.4 첫 번째 강화 학습 알고리즘 구현"],"metadata":{"id":"1w-eSg_pxRlW"}},{"cell_type":"markdown","source":["## 18.4.1 OpenAI 짐 툴킷 소개"],"metadata":{"id":"jkEF9nO_xUVc"}},{"cell_type":"markdown","source":["OpeinAI 짐은 강화 학습 모델을 개발하기 위한 툴킷이다.\n","\n","기본적인 환경으로는 막대의 균현을 잡는 문제와 자동차를 산 위로 이동시키는 방법이 있다."],"metadata":{"id":"uFc4dEQzxYTL"}},{"cell_type":"code","source":["Image(url='https://git.io/JtkcB', width=800)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"id":"DFpbB_uaxqed","executionInfo":{"status":"ok","timestamp":1641966142032,"user_tz":-540,"elapsed":69,"user":{"displayName":"Hwisung Kwon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizs2pMSWME6appADm19iw3b0G9f_CCWjKda6cL=s64","userId":"05107164275958408212"}},"outputId":"ed7d5e2e-1366-4fc4-c4bb-0ed2688e9672"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://git.io/JtkcB\" width=\"800\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["#그리드월드\n","Image(url='https://git.io/Jtkc0', width=800)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"id":"S7f4Axm-xo41","executionInfo":{"status":"ok","timestamp":1641966142034,"user_tz":-540,"elapsed":68,"user":{"displayName":"Hwisung Kwon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gizs2pMSWME6appADm19iw3b0G9f_CCWjKda6cL=s64","userId":"05107164275958408212"}},"outputId":"897f408e-2977-43f8-afbb-3d2d62e78e1c"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://git.io/Jtkc0\" width=\"800\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":9}]}]}